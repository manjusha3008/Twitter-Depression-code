{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import chardet\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from twitter_program import connect_Twitter,tweet_clean_tp,tweet_parse_tp,retrieve_hashtags,retrieve_mentions,retrieve_urls,pos_tags,extract_emojis,spell_check,lemmatise\n",
    "from nltk.stem import PorterStemmer\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Manjusha Pattadkal'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r\"C:\\Users\\Manjusha Pattadkal\\Documents\\ML Project\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "cooccur_dep=pd.read_csv(\"cooccuring_depressed_updated.csv\",encoding='mbcs',header=None)\n",
    "cooccur_nondep=pd.read_csv(\"cooccuring_nd_updated.csv\",encoding='mbcs',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "usernames = pd.read_csv('usernames_depressed.txt',index_col= False,header=None,sep='\\t',error_bad_lines=False)\n",
    "rowcount = len(usernames.axes[0])\n",
    "\n",
    "usernames=usernames.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(String): #default value is always true for stemming and stopwords\n",
    "    \n",
    "    '''\n",
    "    This function is used for preprocessing\n",
    "    - Tokenization\n",
    "    - Stemming\n",
    "    - Stop Words\n",
    "        \n",
    "    '''\n",
    "    tokens = nltk.word_tokenize(String)\n",
    "    token = [word for word in tokens if word.isalpha()]\n",
    "    influential_words = \" \".join(token)\n",
    "    influential_words = influential_words.lower()\n",
    "    influential_words = influential_words.split()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    influentialwords = []\n",
    "    \n",
    "    for w in influential_words:\n",
    "        if w not in stop_words: \n",
    "            influentialwords.append(w)\n",
    "   # stemwords_string = \" \".join(influentialwords)\n",
    "    #words = set(nltk.corpus.words.words())\n",
    "    #sent = stemwords_string\n",
    "   # sent = \" \".join(w for w in nltk.wordpunct_tokenize(sent) if w.lower() in words)\n",
    "    \n",
    "    ps= PorterStemmer()\n",
    "    \n",
    "    # if stopwordFlag==False:\n",
    "    stemwords=[]    \n",
    "    for w in influentialwords:\n",
    "          stemwords.append(ps.stem(w))\n",
    "    stemwords_string = \" \".join(stemwords)\n",
    "    return stemwords_string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data,fun):\n",
    "    data_df=pd.DataFrame(data)\n",
    "    pre = data_df[0].apply(fun)\n",
    "    return pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "usernames=pd.read_csv('usernames_nondepressed.txt',index_col= False,header=None,sep='\\t',error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2solidmiya_'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usernames[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "100",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-84-afcdc82d2ec1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mdep_tweets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mr\"C:\\Users\\Manjusha Pattadkal\\Documents\\ML Project\\non depressed\\2 months\\userdata_\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0musernames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"_2_months.csv\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    869\u001b[0m         \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    870\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 871\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    872\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    873\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_value\u001b[1;34m(self, series, key)\u001b[0m\n\u001b[0;32m   4403\u001b[0m         \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_convert_scalar_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"getitem\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4404\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4405\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtz\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseries\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"tz\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4406\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4407\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mholds_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_boolean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 100"
     ]
    }
   ],
   "source": [
    "dep_tweets = []\n",
    "#dep_vec=[]\n",
    "#nondep_vec = []\n",
    "for i in range (0,rowcount):\n",
    "        dep_tweets = []\n",
    "    \n",
    "        filename = filename=r\"C:\\Users\\Manjusha Pattadkal\\Documents\\ML Project\\non depressed\\2 months\\userdata_\"+usernames[0][i]+\"_2_months.csv\"\n",
    "    \n",
    "    \n",
    "        tweets=pd.read_csv(filename,encoding='mbcs')\n",
    "        tweets.dropna(subset = ['twittername'],inplace=True)\n",
    "\n",
    "        row=len(tweets.axes[0])\n",
    "        \n",
    "        for i in range (0,row):\n",
    "            dep_tweets.append(str(tweets.iloc[i,4]))\n",
    "            \n",
    "        clean = preprocess(dep_tweets,tweet_clean_tp)\n",
    "        tags = preprocess(clean,pos_tags)\n",
    "        nan_value = float(\"NaN\")\n",
    "        tags.replace(\"\",nan_value, inplace=True)\n",
    "        tags.dropna(inplace=True)\n",
    "        tags=tags.reset_index(drop=True)\n",
    "        rowc = len(tags)\n",
    "        final_text = []\n",
    "        for i in range(0,rowc):\n",
    "            final_text.append(str(preprocessing(tags[i])))\n",
    "        x = pd.Series(final_text)\n",
    "\n",
    "        for i in range(len(final_text)):\n",
    "            tokens=final_text[i].split()\n",
    "            if len(tokens)==1:\n",
    "                x.drop(i,inplace=True)\n",
    "\n",
    "        final_text=x.reset_index(drop=True)\n",
    "        DF = {}\n",
    "        for i in range(len(final_text)):\n",
    "            tokens = final_text[i].split()\n",
    "            for w in tokens:\n",
    "                try:\n",
    "                    DF[w].add(i)\n",
    "                except:\n",
    "                    DF[w] = {i}\n",
    "        token_dict = {}\n",
    "        for i in range(len(final_text)):\n",
    "            tokens = final_text[i].split()\n",
    "            #counter = Counter(tokens + final_text[i])\n",
    "            for token in tokens:\n",
    "                #tf = counter[token]/words_count\n",
    "                if token in token_dict.keys():\n",
    "                    token_dict[token] = token_dict[token]+1\n",
    "                else:\n",
    "                    token_dict[token] = 1\n",
    "        freq = int((len(final_text)*0.2)/100)\n",
    "        token_list = []\n",
    "        count = 0\n",
    "        for i in token_dict.keys():\n",
    "             if token_dict[i]>=freq:\n",
    "                    token_list.append(i)\n",
    "                    count+=1\n",
    "        word_list =[]\n",
    "        new_tweets = []\n",
    "        for w in token_list:\n",
    "\n",
    "            for i in range(0,len(final_text)):\n",
    "                word_list=[]\n",
    "                if w in final_text[i].split():\n",
    "\n",
    "                        word_list.append(final_text[i])\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "\n",
    "                if len(word_list) !=0 and word_list not in new_tweets:\n",
    "                    new_tweets.append(word_list)\n",
    "        new_df = pd.DataFrame(new_tweets)\n",
    "        #new_df.to_csv(\"new_tweets.csv\",index = False, header=False)\n",
    "        vector_1,a1 = vectorization(new_tweets,new_tweets)\n",
    "        vector_transpose = np.transpose(vector_1)\n",
    "        vector_tf=tf.convert_to_tensor(vector_1, dtype=None, dtype_hint=None, name=None)\n",
    "        vector_t_tf=tf.convert_to_tensor(vector_transpose, dtype=None, dtype_hint=None, name=None)\n",
    "\n",
    "        result = tf.matmul(vector_t_tf,vector_tf)\n",
    "        result_arr=result.numpy()\n",
    "        index_ = []\n",
    "        for i in range (len(a1)):\n",
    "             for j in range (0,len(a1)):\n",
    "                if result_arr[i][j] >0 and j<i:\n",
    "                    index_.append([result_arr[i][j],i,j])\n",
    "        list_freq = []\n",
    "        for i in range (len(index_)):\n",
    "            list_freq.append(index_[i][0])\n",
    "        x = np.array(list_freq) \n",
    "        f = (max(np.unique(x)) * 10)/100\n",
    "        index_ = []\n",
    "        for i in range (len(a1)):\n",
    "             for j in range (0,len(a1)):\n",
    "                if result_arr[i][j] >int(f) and j<i:\n",
    "                    index_.append([result_arr[i][j],i,j])\n",
    "        co_occur = []\n",
    "        for i in range (len(index_)):\n",
    "            a = index_[i][1]\n",
    "            b = index_[i][2]\n",
    "            co_occur.append([a1[a],a1[b],index_[i][0]])\n",
    "        count_dep = 0\n",
    "        for i in range(0,len(co_occur)):\n",
    "            a = co_occur[i][0]\n",
    "            b=co_occur[i][1]\n",
    "            for j in range(0,len(cooccur_dep)):\n",
    "                if a == cooccur_dep[0][j] and b == cooccur_dep[1][j]:\n",
    "                    count_dep = count_dep + 1\n",
    "        count_ndep = 0\n",
    "        for i in range(0,len(co_occur)):\n",
    "            a = co_occur[i][0]\n",
    "            b=co_occur[i][1]\n",
    "            for j in range(0,len(cooccur_nondep)):\n",
    "                if a == cooccur_nondep[0][j] and b == cooccur_nondep[1][j]:\n",
    "                    count_ndep = count_ndep + 1\n",
    "        \n",
    "        dep_vec.append((1865/len(co_occur))*(count_dep/2215))\n",
    "        nondep_vec.append((500.55/len(co_occur))*(count_ndep/638))\n",
    "        cat.append(0)\n",
    "                         \n",
    "          \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_df = pd.DataFrame(zip(dep_vec,nondep_vec,cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_df.to_csv(r\"cooccurences_vector.csv\",header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = [1] * len(dep_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_freq(token):\n",
    "    return len(DF[token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorization(Stringfit,Stringtransform):\n",
    "    count = []\n",
    "    vectorizer = CountVectorizer(min_df=freq)\n",
    "    String_df = pd.DataFrame(Stringfit)\n",
    "    Stringtransform_df = pd.DataFrame(Stringtransform)\n",
    "    vectorizer.fit(String_df[0])\n",
    "    countmatrix = vectorizer.transform(Stringtransform_df[0])\n",
    "    count=countmatrix.todense()\n",
    "    return count,vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
